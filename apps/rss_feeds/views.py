import base64
import datetime
import time
from collections import defaultdict
from urllib.parse import urlparse

import redis
import requests
from django.conf import settings
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.http import (
    Http404,
    HttpResponse,
    HttpResponseForbidden,
    HttpResponseRedirect,
)
from django.shortcuts import get_object_or_404, render
from django.views.decorators.http import condition

from apps.analyzer.models import get_classifiers_for_user
from apps.push.models import PushSubscription
from apps.reader.models import UserSubscription

# from django.db import IntegrityError
from apps.rss_feeds.models import Feed, MFeedIcon, MFetchHistory, MStory, merge_feeds
from apps.search.models import MUserSearch
from apps.statistics.rtrending_subscriptions import RTrendingSubscription
from utils import feedfinder_forman as feedfinder
from utils import json_functions as json
from utils import log as logging
from utils.feed_functions import relative_timesince, relative_timeuntil
from utils.ratelimit import ratelimit
from utils.user_functions import ajax_login_required, get_user
from utils.view_functions import get_argument_or_404, is_true, required_params
from vendor.timezones.utilities import localtime_for_timezone

IGNORE_AUTOCOMPLETE = [
    "facebook.com/feeds/notifications.php",
    "inbox",
    "secret",
    "password",
    "latitude",
]


@ajax_login_required
@json.json_view
def search_feed(request):
    address = request.GET.get("address")
    offset = int(request.GET.get("offset", 0))
    if not address:
        return dict(code=-1, message="Please provide a URL/address.")

    logging.user(request.user, "~FBFinding feed (search_feed): %s" % address)
    ip = request.META.get("HTTP_X_FORWARDED_FOR", None) or request.META["REMOTE_ADDR"]
    logging.user(request.user, "~FBIP: %s" % ip)
    aggressive = request.user.is_authenticated
    feed = Feed.get_feed_from_url(address, create=False, aggressive=aggressive, offset=offset)
    if feed:
        return feed.canonical()
    else:
        return dict(code=-1, message="No feed found matching that XML or website address.")


@json.json_view
def load_single_feed(request, feed_id):
    user = get_user(request)
    feed = get_object_or_404(Feed, pk=feed_id)
    classifiers = get_classifiers_for_user(user, feed_id=feed.pk)

    payload = feed.canonical(full=True)
    payload["classifiers"] = classifiers

    return payload


def feed_favicon_etag(request, feed_id):
    try:
        feed_icon = MFeedIcon.objects.get(feed_id=feed_id)
    except MFeedIcon.DoesNotExist:
        return

    return feed_icon.color


@condition(etag_func=feed_favicon_etag)
def load_feed_favicon(request, feed_id):
    not_found = False
    try:
        feed_icon = MFeedIcon.objects.get(feed_id=feed_id)
    except MFeedIcon.DoesNotExist:
        logging.user(request, "~FBNo feed icon found: %s" % feed_id)
        not_found = True

    if not_found or not feed_icon.data:
        return HttpResponseRedirect(settings.MEDIA_URL + "img/icons/nouns/world.svg")

    icon_data = base64.b64decode(feed_icon.data)
    return HttpResponse(icon_data, content_type="image/png")


@json.json_view
def feed_autocomplete(request):
    query = request.GET.get("term") or request.GET.get("query")
    version = int(request.GET.get("v", 1))
    autocomplete_format = request.GET.get("format", "autocomplete")

    # user = get_user(request)
    # if True or not user.profile.is_premium:
    #     return dict(code=-1, message="Overloaded, no autocomplete results.", feeds=[], term=query)

    if not query:
        return dict(code=-1, message="Specify a search 'term'.", feeds=[], term=query)

    if "." in query:
        try:
            parts = urlparse(query)
            if not parts.hostname and not query.startswith("http"):
                parts = urlparse("http://%s" % query)
            if parts.hostname:
                query = [parts.hostname]
                query.extend([p for p in parts.path.split("/") if p])
                query = " ".join(query)
        except:
            logging.user(request, "~FGAdd search, could not parse url in ~FR%s" % query)

    query_params = query.split(" ")
    tries_left = 5
    while len(query_params) and tries_left:
        tries_left -= 1
        feed_ids = Feed.autocomplete(" ".join(query_params))
        if feed_ids:
            break
        else:
            query_params = query_params[:-1]

    feeds = list(set([Feed.get_by_id(feed_id) for feed_id in feed_ids]))
    feeds = [feed for feed in feeds if feed and not feed.branch_from_feed]
    feeds = [feed for feed in feeds if all([x not in feed.feed_address for x in IGNORE_AUTOCOMPLETE])]

    if autocomplete_format == "autocomplete":
        feeds = [
            {
                "id": feed.pk,
                "value": feed.feed_address,
                "label": feed.feed_title,
                "tagline": feed.data and feed.data.feed_tagline,
                "num_subscribers": feed.num_subscribers,
            }
            for feed in feeds
        ]
    else:
        feeds = [feed.canonical(full=True) for feed in feeds]
    feeds = sorted(feeds, key=lambda f: -1 * f["num_subscribers"])

    feed_ids = [f["id"] for f in feeds]
    feed_icons = dict((icon.feed_id, icon) for icon in MFeedIcon.objects.filter(feed_id__in=feed_ids))

    for feed in feeds:
        if feed["id"] in feed_icons:
            feed_icon = feed_icons[feed["id"]]
            if feed_icon.data:
                feed["favicon_color"] = feed_icon.color
                feed["favicon"] = feed_icon.data

    logging.user(
        request,
        "~FGAdd Search: ~SB%s ~SN(%s matches)"
        % (
            query,
            len(feeds),
        ),
    )

    if version > 1:
        return {
            "feeds": feeds,
            "term": query,
        }
    else:
        return feeds


@ratelimit(minutes=1, requests=30)
@json.json_view
def load_feed_statistics(request, feed_id):
    user = get_user(request)
    feed = get_object_or_404(Feed, pk=feed_id)
    stats = assemble_statistics(user, feed_id)

    logging.user(request, "~FBStatistics: ~SB%s" % (feed))

    return stats


def load_feed_statistics_embedded(request, feed_id):
    user = get_user(request)
    feed = get_object_or_404(Feed, pk=feed_id)
    stats = assemble_statistics(user, feed_id)

    logging.user(request, "~FBStatistics (~FCembedded~FB): ~SB%s" % (feed))

    return render(
        request,
        "rss_feeds/statistics.xhtml",
        {
            "stats": json.json_encode(stats),
            "feed_js": json.json_encode(feed.canonical()),
            "feed": feed,
        },
    )


def assemble_statistics(user, feed_id):
    user_timezone = user.profile.timezone
    stats = dict()
    feed = get_object_or_404(Feed, pk=feed_id)
    feed.update_all_statistics()
    feed.set_next_scheduled_update(verbose=True, skip_scheduling=True)
    feed.save_feed_story_history_statistics()
    feed.save_classifier_counts()

    # Dates of last and next update
    stats["active"] = feed.active
    stats["last_update"] = relative_timesince(feed.last_update)
    stats["next_update"] = relative_timeuntil(feed.next_scheduled_update)
    stats["push"] = feed.is_push
    stats["fs_size_bytes"] = feed.fs_size_bytes
    stats["archive_count"] = feed.archive_count
    if feed.is_push:
        try:
            stats["push_expires"] = localtime_for_timezone(feed.push.lease_expires, user_timezone).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
        except PushSubscription.DoesNotExist:
            stats["push_expires"] = "Missing push"
            feed.is_push = False
            feed.save()

    # Minutes between updates
    update_interval_minutes = feed.get_next_scheduled_update(force=True, verbose=False)
    stats["update_interval_minutes"] = update_interval_minutes
    original_active_premium_subscribers = feed.active_premium_subscribers
    original_premium_subscribers = feed.premium_subscribers
    feed.active_premium_subscribers = max(feed.active_premium_subscribers + 1, 1)
    feed.premium_subscribers += 1
    premium_update_interval_minutes = feed.get_next_scheduled_update(
        force=True, verbose=False, premium_speed=True
    )
    feed.active_premium_subscribers = original_active_premium_subscribers
    feed.premium_subscribers = original_premium_subscribers
    stats["premium_update_interval_minutes"] = premium_update_interval_minutes
    stats["errors_since_good"] = feed.errors_since_good

    # Stories per month - average and month-by-month breakout
    average_stories_per_month, story_count_history = (
        feed.average_stories_per_month,
        feed.data.story_count_history,
    )
    stats["average_stories_per_month"] = average_stories_per_month
    story_count_history = story_count_history and json.decode(story_count_history)
    if story_count_history and isinstance(story_count_history, dict):
        stats["story_count_history"] = story_count_history["months"]
        stats["story_days_history"] = story_count_history["days"]
        stats["story_hours_history"] = story_count_history["hours"]
    else:
        stats["story_count_history"] = story_count_history

    # Rotate hours to match user's timezone offset
    localoffset = user_timezone.utcoffset(datetime.datetime.utcnow())
    hours_offset = int(localoffset.total_seconds() / 3600)
    rotated_hours = {}
    for hour, value in list(stats["story_hours_history"].items()):
        rotated_hours[str(int(hour) + hours_offset)] = value
    stats["story_hours_history"] = rotated_hours

    # Subscribers
    stats["subscriber_count"] = feed.num_subscribers
    stats["num_subscribers"] = feed.num_subscribers
    stats["stories_last_month"] = feed.stories_last_month
    stats["last_load_time"] = feed.last_load_time
    stats["premium_subscribers"] = feed.premium_subscribers
    stats["active_subscribers"] = feed.active_subscribers
    stats["active_premium_subscribers"] = feed.active_premium_subscribers

    # Classifier counts
    stats["classifier_counts"] = json.decode(feed.data.feed_classifier_counts)

    # Fetch histories
    fetch_history = MFetchHistory.feed(feed_id, timezone=user_timezone)
    stats["feed_fetch_history"] = fetch_history["feed_fetch_history"]
    stats["page_fetch_history"] = fetch_history["page_fetch_history"]
    stats["feed_push_history"] = fetch_history["push_history"]

    return stats


@json.json_view
def load_feed_settings(request, feed_id):
    stats = dict()
    feed = get_object_or_404(Feed, pk=feed_id)
    user = get_user(request)
    timezone = user.profile.timezone

    fetch_history = MFetchHistory.feed(feed_id, timezone=timezone)
    stats["feed_fetch_history"] = fetch_history["feed_fetch_history"]
    stats["page_fetch_history"] = fetch_history["page_fetch_history"]
    stats["feed_push_history"] = fetch_history["push_history"]
    stats["duplicate_addresses"] = feed.duplicate_addresses.all()

    return stats


@ratelimit(minutes=1, requests=30)
@json.json_view
def exception_retry(request):
    user = get_user(request)
    feed_id = get_argument_or_404(request, "feed_id")
    reset_fetch = json.decode(request.POST["reset_fetch"])
    feed = Feed.get_by_id(feed_id)
    original_feed = feed

    if not feed:
        raise Http404

    feed.schedule_feed_fetch_immediately()
    changed = False
    if feed.has_page_exception:
        changed = True
        feed.has_page_exception = False
    if feed.has_feed_exception:
        changed = True
        feed.has_feed_exception = False
    if not feed.active:
        changed = True
        feed.active = True
    if changed:
        feed.save(update_fields=["has_page_exception", "has_feed_exception", "active"])

    original_fetched_once = feed.fetched_once
    if reset_fetch:
        logging.user(request, "~FRRefreshing exception feed: ~SB%s" % (feed))
        feed.fetched_once = False
    else:
        logging.user(request, "~FRForcing refreshing feed: ~SB%s" % (feed))

        feed.fetched_once = True
    if feed.fetched_once != original_fetched_once:
        feed.save(update_fields=["fetched_once"])

    feed = feed.update(force=True, compute_scores=False, verbose=True)
    feed = Feed.get_by_id(feed.pk)

    try:
        usersub = UserSubscription.objects.get(user=user, feed=feed)
    except UserSubscription.DoesNotExist:
        usersubs = UserSubscription.objects.filter(user=user, feed=original_feed)
        if usersubs:
            usersub = usersubs[0]
            usersub.switch_feed(feed, original_feed)
        else:
            return {"code": -1}
    usersub.calculate_feed_scores(silent=False)

    feeds = {feed.pk: usersub and usersub.canonical(full=True), feed_id: usersub.canonical(full=True)}
    return {"code": 1, "feeds": feeds}


@ajax_login_required
@json.json_view
def exception_change_feed_address(request):
    feed_id = request.POST["feed_id"]
    feed = get_object_or_404(Feed, pk=feed_id)
    original_feed = feed
    feed_address = request.POST["feed_address"]
    timezone = request.user.profile.timezone
    code = -1

    if False and (feed.has_page_exception or feed.has_feed_exception):
        # Fix broken feed
        logging.user(
            request,
            "~FRFixing feed exception by address: %s - ~SB%s~SN to ~SB%s"
            % (feed, feed.feed_address, feed_address),
        )
        feed.has_feed_exception = False
        feed.active = True
        feed.fetched_once = False
        feed.feed_address = feed_address
        duplicate_feed = feed.schedule_feed_fetch_immediately()
        code = 1
        if duplicate_feed:
            new_feed = Feed.objects.get(pk=duplicate_feed.pk)
            feed = new_feed
            new_feed.schedule_feed_fetch_immediately()
            new_feed.has_feed_exception = False
            new_feed.active = True
            new_feed = new_feed.save()
            if new_feed.pk != feed.pk:
                merge_feeds(new_feed.pk, feed.pk)
    else:
        # Branch good feed
        logging.user(
            request, "~FRBranching feed by address: ~SB%s~SN to ~SB%s" % (feed.feed_address, feed_address)
        )
        try:
            feed = Feed.objects.get(
                hash_address_and_link=Feed.generate_hash_address_and_link(feed_address, feed.feed_link)
            )
        except Feed.DoesNotExist:
            feed = Feed.objects.create(feed_address=feed_address, feed_link=feed.feed_link)
        code = 1
        if feed.pk != original_feed.pk:
            try:
                feed.branch_from_feed = original_feed.branch_from_feed or original_feed
            except Feed.DoesNotExist:
                feed.branch_from_feed = original_feed
            feed.feed_address_locked = True
            feed = feed.save()

    feed = feed.update()
    feed = Feed.get_by_id(feed.pk)
    try:
        usersub = UserSubscription.objects.get(user=request.user, feed=feed)
    except UserSubscription.DoesNotExist:
        usersubs = UserSubscription.objects.filter(user=request.user, feed=original_feed)
        if usersubs:
            usersub = usersubs[0]
            usersub.switch_feed(feed, original_feed)
        else:
            fetch_history = MFetchHistory.feed(feed_id, timezone=timezone)
            return {
                "code": -1,
                "feed_fetch_history": fetch_history["feed_fetch_history"],
                "page_fetch_history": fetch_history["page_fetch_history"],
                "push_history": fetch_history["push_history"],
            }

    usersub.calculate_feed_scores(silent=False)

    feed.update_all_statistics()
    classifiers = get_classifiers_for_user(usersub.user, feed_id=usersub.feed_id)

    feeds = {
        original_feed.pk: usersub and usersub.canonical(full=True, classifiers=classifiers),
    }

    if feed and feed.has_feed_exception:
        code = -1

    fetch_history = MFetchHistory.feed(feed_id, timezone=timezone)
    return {
        "code": code,
        "feeds": feeds,
        "new_feed_id": usersub.feed_id,
        "feed_fetch_history": fetch_history["feed_fetch_history"],
        "page_fetch_history": fetch_history["page_fetch_history"],
        "push_history": fetch_history["push_history"],
    }


@ajax_login_required
@json.json_view
def exception_change_feed_link(request):
    feed_id = request.POST["feed_id"]
    feed = get_object_or_404(Feed, pk=feed_id)
    original_feed = feed
    feed_link = request.POST["feed_link"]
    timezone = request.user.profile.timezone
    code = -1

    if False and (feed.has_page_exception or feed.has_feed_exception):
        # Fix broken feed
        logging.user(
            request, "~FRFixing feed exception by link: ~SB%s~SN to ~SB%s" % (feed.feed_link, feed_link)
        )
        found_feed_urls = feedfinder.find_feeds(feed_link)
        if len(found_feed_urls):
            code = 1
            feed.has_page_exception = False
            feed.active = True
            feed.fetched_once = False
            feed.feed_link = feed_link
            feed.feed_address = found_feed_urls[0]
            duplicate_feed = feed.schedule_feed_fetch_immediately()
            if duplicate_feed:
                new_feed = Feed.objects.get(pk=duplicate_feed.pk)
                feed = new_feed
                new_feed.schedule_feed_fetch_immediately()
                new_feed.has_page_exception = False
                new_feed.active = True
                new_feed.save()
    else:
        # Branch good feed
        logging.user(request, "~FRBranching feed by link: ~SB%s~SN to ~SB%s" % (feed.feed_link, feed_link))
        try:
            feed = Feed.objects.get(
                hash_address_and_link=Feed.generate_hash_address_and_link(feed.feed_address, feed_link)
            )
        except Feed.DoesNotExist:
            feed = Feed.objects.create(feed_address=feed.feed_address, feed_link=feed_link)
        code = 1
        if feed.pk != original_feed.pk:
            try:
                feed.branch_from_feed = original_feed.branch_from_feed or original_feed
            except Feed.DoesNotExist:
                feed.branch_from_feed = original_feed
            feed.feed_link_locked = True
            feed.save()

    feed = feed.update()
    feed = Feed.get_by_id(feed.pk)

    try:
        usersub = UserSubscription.objects.get(user=request.user, feed=feed)
    except UserSubscription.DoesNotExist:
        usersubs = UserSubscription.objects.filter(user=request.user, feed=original_feed)
        if usersubs:
            usersub = usersubs[0]
            usersub.switch_feed(feed, original_feed)
        else:
            fetch_history = MFetchHistory.feed(feed_id, timezone=timezone)
            return {
                "code": -1,
                "feed_fetch_history": fetch_history["feed_fetch_history"],
                "page_fetch_history": fetch_history["page_fetch_history"],
                "push_history": fetch_history["push_history"],
            }

    usersub.calculate_feed_scores(silent=False)

    feed.update_all_statistics()
    classifiers = get_classifiers_for_user(usersub.user, feed_id=usersub.feed_id)

    if feed and feed.has_feed_exception:
        code = -1

    feeds = {
        original_feed.pk: usersub.canonical(full=True, classifiers=classifiers),
    }
    fetch_history = MFetchHistory.feed(feed_id, timezone=timezone)
    return {
        "code": code,
        "feeds": feeds,
        "new_feed_id": usersub.feed_id,
        "feed_fetch_history": fetch_history["feed_fetch_history"],
        "page_fetch_history": fetch_history["page_fetch_history"],
        "push_history": fetch_history["push_history"],
    }


@login_required
def status(request):
    if not request.user.is_staff and not settings.DEBUG:
        logging.user(request, "~SKNON-STAFF VIEWING RSS FEEDS STATUS!")
        assert False
        return HttpResponseForbidden()
    minutes = int(request.GET.get("minutes", 1))
    now = datetime.datetime.now()
    hour_ago = now + datetime.timedelta(minutes=minutes)
    username = request.GET.get("user", "") or request.GET.get("username", "")
    if username == "all":
        feeds = Feed.objects.filter(next_scheduled_update__lte=hour_ago).order_by("next_scheduled_update")
    else:
        if username:
            user = User.objects.get(username=username)
        else:
            user = request.user
        usersubs = UserSubscription.objects.filter(user=user)
        feed_ids = usersubs.values("feed_id")
        if minutes > 0:
            feeds = Feed.objects.filter(pk__in=feed_ids, next_scheduled_update__lte=hour_ago).order_by(
                "next_scheduled_update"
            )
        else:
            feeds = Feed.objects.filter(pk__in=feed_ids, last_update__gte=hour_ago).order_by("-last_update")

    r = redis.Redis(connection_pool=settings.REDIS_FEED_UPDATE_POOL)
    queues = {
        "tasked_feeds": r.zcard("tasked_feeds"),
        "queued_feeds": r.scard("queued_feeds"),
        "scheduled_updates": r.zcard("scheduled_updates"),
    }
    return render(request, "rss_feeds/status.xhtml", {"feeds": feeds, "queues": queues})


@ratelimit(minutes=1, requests=30)
@json.json_view
def original_text(request):
    # iOS sends a POST, web sends a GET
    GET_POST = getattr(request, request.method)
    story_id = GET_POST.get("story_id")
    feed_id = GET_POST.get("feed_id")
    story_hash = GET_POST.get("story_hash", None)
    force = GET_POST.get("force", False)
    debug = GET_POST.get("debug", False)

    if not story_hash and not story_id:
        return {"code": -1, "message": "Missing story_hash.", "original_text": None, "failed": True}

    if story_hash:
        story, _ = MStory.find_story(story_hash=story_hash)
    else:
        story, _ = MStory.find_story(story_id=story_id, story_feed_id=feed_id)

    if not story:
        logging.user(request, "~FYFetching ~FGoriginal~FY story text: ~FRstory not found")
        return {"code": -1, "message": "Story not found.", "original_text": None, "failed": True}

    original_text = story.fetch_original_text(force=force, request=request, debug=debug)

    return {
        "feed_id": story.story_feed_id,
        "story_hash": story.story_hash,
        "story_id": story.story_guid,
        "image_urls": story.image_urls,
        "secure_image_urls": Feed.secure_image_urls(story.image_urls),
        "original_text": original_text,
        "failed": not original_text or len(original_text) < 100,
    }


@required_params("story_hash", method="GET")
def original_story(request):
    story_hash = request.GET.get("story_hash")
    force = request.GET.get("force", False)
    debug = request.GET.get("debug", False)

    story, _ = MStory.find_story(story_hash=story_hash)

    if not story:
        logging.user(request, "~FYFetching ~FGoriginal~FY story page: ~FRstory not found")
        # return {'code': -1, 'message': 'Story not found.', 'original_page': None, 'failed': True}
        raise Http404

    original_page = story.fetch_original_page(force=force, request=request, debug=debug)

    return HttpResponse(original_page or "")


@required_params("story_hash", method="GET")
@json.json_view
def story_changes(request):
    story_hash = request.GET.get("story_hash", None)
    show_changes = is_true(request.GET.get("show_changes", True))
    story, _ = MStory.find_story(story_hash=story_hash)
    if not story:
        logging.user(request, "~FYFetching ~FGoriginal~FY story page: ~FRstory not found")
        return {"code": -1, "message": "Story not found.", "original_page": None, "failed": True}

    return {"story": Feed.format_story(story, show_changes=show_changes)}


@ajax_login_required
@json.json_view
def discover_feeds(request, feed_id=None):
    page = int(request.GET.get("page") or request.POST.get("page") or 1)
    limit = 5
    offset = (page - 1) * limit

    if request.method == "GET" and feed_id:
        similar_feed_ids = list(
            Feed.get_by_id(feed_id)
            .count_similar_feeds(force=True, offset=offset, limit=limit)
            .values_list("pk", flat=True)
        )
    elif request.method == "POST":
        feed_ids = request.POST.getlist("feed_ids")
        similar_feeds = Feed.find_similar_feeds(feed_ids=feed_ids, offset=offset, limit=limit)
        similar_feed_ids = [result["_source"]["feed_id"] for result in similar_feeds]
    else:
        return {"code": -1, "message": "Missing feed_ids.", "discover_feeds": None, "failed": True}

    feeds = Feed.objects.filter(pk__in=similar_feed_ids)
    discover_feeds = defaultdict(dict)
    for feed in feeds:
        discover_feeds[feed.pk]["feed"] = feed.canonical(include_favicon=False)
        discover_feeds[feed.pk]["stories"] = feed.get_stories(limit=5)

    logging.user(request, "~FCDiscovering similar feeds, page %s: ~SB%s" % (page, similar_feed_ids))
    return {"discover_feeds": discover_feeds}


@ajax_login_required
@json.json_view
def discover_stories(request, story_hash):
    page = int(request.GET.get("page") or request.POST.get("page") or 1)
    feed_ids = request.GET.getlist("feed_ids") or request.POST.getlist("feed_ids")
    limit = 5
    offset = (page - 1) * limit
    story, _ = MStory.find_story(story_hash=story_hash)
    if not story:
        return {"code": -1, "message": "Story not found.", "discover_stories": None, "failed": True}

    user_search = MUserSearch.get_user(request.user.pk)
    user_search.touch_discover_date()

    similar_stories = story.fetch_similar_stories(feed_ids=feed_ids, offset=offset, limit=limit)
    similar_story_hashes = [result["_id"] for result in similar_stories]
    stories = MStory.objects.filter(story_hash__in=similar_story_hashes)
    stories = Feed.format_stories(stories)

    # Find unsubscribed feeds
    subscribed_feed_ids = UserSubscription.objects.filter(
        user=request.user, feed_id__in=set(story["story_feed_id"] for story in stories)
    ).values_list("feed_id", flat=True)
    feeds = Feed.objects.filter(
        pk__in=set(story["story_feed_id"] for story in stories) - set(subscribed_feed_ids)
    )
    feeds = {feed.pk: feed.canonical(include_favicon=False) for feed in feeds}

    return {"discover_stories": stories, "feeds": feeds}


@json.json_view
def trending_sites(request):
    """
    Returns trending feeds with their recent stories for the Trending Sites feature.
    Uses subscription velocity from RTrendingSubscription to determine trending feeds.
    """
    page = int(request.GET.get("page", 1))
    days = int(request.GET.get("days", 7))
    limit = 10
    offset = (page - 1) * limit

    # Validate days parameter
    if days not in [1, 7, 30]:
        days = 7

    # Get trending feed IDs from subscription velocity
    # In DEBUG mode, use min_subscribers=1 to show results with limited data
    min_subs = 1 if settings.DEBUG else RTrendingSubscription.MIN_SUBSCRIBERS_THRESHOLD
    trending_data = RTrendingSubscription.get_trending_feeds(
        days=days,
        limit=limit + offset + 10,  # Get extra to account for filtering
        min_subscribers=min_subs,
    )

    # Slice for pagination
    trending_feed_ids = [int(feed_id) for feed_id, score in trending_data[offset : offset + limit]]

    if not trending_feed_ids:
        return {"trending_feeds": {}, "has_more": False}

    # Build response with feed details and stories
    feeds = Feed.objects.filter(pk__in=trending_feed_ids)
    feeds_dict = {feed.pk: feed for feed in feeds}

    # Build ordered response preserving trending order
    trending_feeds = {}
    for feed_id in trending_feed_ids:
        if feed_id in feeds_dict:
            feed = feeds_dict[feed_id]
            # Find score for this feed
            score = next((s for fid, s in trending_data if int(fid) == feed_id), 0)
            trending_feeds[feed_id] = {
                "feed": feed.canonical(include_favicon=False),
                "stories": feed.get_stories(limit=5),
                "trending_score": score,
            }

    has_more = len(trending_data) > offset + limit

    logging.user(request, "~FCTrending sites (page %s, %sd): ~SB%s feeds" % (page, days, len(trending_feeds)))
    return {"trending_feeds": trending_feeds, "has_more": has_more}


@json.json_view
def youtube_search(request):
    """
    Search YouTube channels and playlists using the YouTube Data API v3.
    Returns results with constructed RSS feed URLs.
    """
    query = request.GET.get("query", "").strip()
    search_type = request.GET.get("type", "channel")  # 'channel' or 'playlist'
    max_results = min(int(request.GET.get("limit", 10)), 25)

    if not query:
        return {"code": -1, "message": "Please provide a search query.", "results": []}

    if not settings.YOUTUBE_API_KEY or settings.YOUTUBE_API_KEY == "YOUR_YOUTUBE_API_KEY":
        return {"code": -1, "message": "YouTube API key not configured.", "results": []}

    # Build YouTube Data API v3 search URL
    api_url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "q": query,
        "type": search_type,
        "maxResults": max_results,
        "key": settings.YOUTUBE_API_KEY,
    }

    try:
        response = requests.get(api_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
    except requests.exceptions.RequestException as e:
        logging.user(request, "~FRYouTube search error: %s" % str(e))
        return {"code": -1, "message": "YouTube API request failed.", "results": []}

    if "error" in data:
        error_msg = data["error"].get("message", "Unknown error")
        logging.user(request, "~FRYouTube API error: %s" % error_msg)
        return {"code": -1, "message": error_msg, "results": []}

    results = []
    for item in data.get("items", []):
        snippet = item.get("snippet", {})
        thumbnails = snippet.get("thumbnails", {})
        thumbnail_url = thumbnails.get("medium", {}).get("url") or thumbnails.get("default", {}).get("url", "")

        if search_type == "channel":
            channel_id = item.get("id", {}).get("channelId")
            if channel_id:
                feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
                channel_url = f"https://www.youtube.com/channel/{channel_id}"
                results.append({
                    "id": channel_id,
                    "type": "channel",
                    "title": snippet.get("channelTitle") or snippet.get("title", ""),
                    "description": snippet.get("description", ""),
                    "thumbnail": thumbnail_url,
                    "feed_url": feed_url,
                    "link": channel_url,
                })
        elif search_type == "playlist":
            playlist_id = item.get("id", {}).get("playlistId")
            if playlist_id:
                feed_url = f"https://www.youtube.com/feeds/videos.xml?playlist_id={playlist_id}"
                playlist_url = f"https://www.youtube.com/playlist?list={playlist_id}"
                results.append({
                    "id": playlist_id,
                    "type": "playlist",
                    "title": snippet.get("title", ""),
                    "description": snippet.get("description", ""),
                    "thumbnail": thumbnail_url,
                    "feed_url": feed_url,
                    "link": playlist_url,
                    "channel_title": snippet.get("channelTitle", ""),
                })

    logging.user(request, "~FBYouTube search for '%s' (%s): ~SB%s results" % (query, search_type, len(results)))
    return {"code": 1, "results": results}


@json.json_view
def reddit_search(request):
    """
    Search Reddit subreddits using the public Reddit JSON API.
    Returns results with constructed RSS feed URLs.
    """
    query = request.GET.get("query", "").strip()
    limit = min(int(request.GET.get("limit", 15)), 25)

    if not query:
        return {"code": -1, "message": "Please provide a search query.", "results": []}

    # Reddit's public JSON API for subreddit search
    api_url = "https://www.reddit.com/subreddits/search.json"
    params = {
        "q": query,
        "limit": limit,
        "include_over_18": "false",
    }
    headers = {
        "User-Agent": "NewsBlur/1.0 (RSS Reader; https://newsblur.com)",
    }

    try:
        response = requests.get(api_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
    except requests.exceptions.RequestException as e:
        logging.user(request, "~FRReddit search error: %s" % str(e))
        return {"code": -1, "message": "Reddit API request failed.", "results": []}

    results = []
    for child in data.get("data", {}).get("children", []):
        subreddit = child.get("data", {})
        name = subreddit.get("display_name", "")
        if name:
            feed_url = f"https://www.reddit.com/r/{name}/.rss"
            subreddit_url = f"https://www.reddit.com/r/{name}"

            # Get icon - try community_icon first, then icon_img
            icon_url = subreddit.get("community_icon", "")
            if icon_url:
                # Remove query params that may cause issues
                icon_url = icon_url.split("?")[0]
            if not icon_url:
                icon_url = subreddit.get("icon_img", "")

            results.append({
                "id": subreddit.get("id", ""),
                "name": name,
                "title": subreddit.get("title", name),
                "description": subreddit.get("public_description", "")[:200],
                "subscribers": subreddit.get("subscribers", 0),
                "icon": icon_url,
                "feed_url": feed_url,
                "link": subreddit_url,
                "over18": subreddit.get("over18", False),
            })

    logging.user(request, "~FBReddit search for '%s': ~SB%s results" % (query, len(results)))
    return {"code": 1, "results": results}


@json.json_view
def reddit_popular(request):
    """
    Get popular Reddit subreddits.
    Returns results with constructed RSS feed URLs.
    """
    limit = min(int(request.GET.get("limit", 20)), 50)

    # Reddit's public JSON API for popular subreddits
    api_url = "https://www.reddit.com/subreddits/popular.json"
    params = {
        "limit": limit,
    }
    headers = {
        "User-Agent": "NewsBlur/1.0 (RSS Reader; https://newsblur.com)",
    }

    try:
        response = requests.get(api_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
    except requests.exceptions.RequestException as e:
        logging.user(request, "~FRReddit popular error: %s" % str(e))
        return {"code": -1, "message": "Reddit API request failed.", "results": []}

    results = []
    for child in data.get("data", {}).get("children", []):
        subreddit = child.get("data", {})
        name = subreddit.get("display_name", "")
        if name and not subreddit.get("over18", False):
            feed_url = f"https://www.reddit.com/r/{name}/.rss"
            subreddit_url = f"https://www.reddit.com/r/{name}"

            # Get icon - try community_icon first, then icon_img
            icon_url = subreddit.get("community_icon", "")
            if icon_url:
                icon_url = icon_url.split("?")[0]
            if not icon_url:
                icon_url = subreddit.get("icon_img", "")

            results.append({
                "id": subreddit.get("id", ""),
                "name": name,
                "title": subreddit.get("title", name),
                "description": subreddit.get("public_description", "")[:200],
                "subscribers": subreddit.get("subscribers", 0),
                "icon": icon_url,
                "feed_url": feed_url,
                "link": subreddit_url,
            })

    logging.user(request, "~FBReddit popular: ~SB%s results" % len(results))
    return {"code": 1, "results": results}


@json.json_view
def newsletter_convert(request):
    """
    Convert a newsletter URL to its RSS feed URL.
    Supports Substack, Medium, Ghost, and other common platforms.
    """
    url = request.GET.get("url", "").strip()

    if not url:
        return {"code": -1, "message": "Please provide a newsletter URL.", "feed_url": None}

    # Normalize URL
    if not url.startswith(("http://", "https://")):
        url = "https://" + url

    # Remove trailing slash
    url = url.rstrip("/")

    feed_url = None
    platform = None
    title = None

    # Parse the URL
    from urllib.parse import urlparse

    parsed = urlparse(url)
    hostname = parsed.netloc.lower()
    path = parsed.path

    # Substack detection
    if "substack.com" in hostname or hostname.endswith(".substack.com"):
        # Extract subdomain for substack
        if hostname.endswith(".substack.com"):
            subdomain = hostname.replace(".substack.com", "")
            feed_url = f"https://{subdomain}.substack.com/feed"
            title = subdomain.replace("-", " ").title()
        else:
            # Could be a custom domain pointing to substack
            feed_url = f"{url}/feed"
        platform = "substack"

    # Medium detection
    elif "medium.com" in hostname:
        if path.startswith("/@"):
            # User profile: medium.com/@username
            username = path.split("/")[1]
            feed_url = f"https://medium.com/feed/{username}"
            title = username.lstrip("@")
        elif path.startswith("/"):
            # Publication: medium.com/publication-name
            publication = path.split("/")[1] if len(path.split("/")) > 1 else ""
            if publication and publication != "":
                feed_url = f"https://medium.com/feed/{publication}"
                title = publication.replace("-", " ").title()
        platform = "medium"

    # Ghost blogs (common pattern)
    elif any(ghost_indicator in url.lower() for ghost_indicator in [".ghost.io", "/ghost/"]):
        feed_url = f"{url}/rss/"
        platform = "ghost"

    # Buttondown detection
    elif "buttondown.email" in hostname:
        # buttondown.email/username
        username = path.lstrip("/").split("/")[0] if path else ""
        if username:
            feed_url = f"https://buttondown.email/{username}/rss"
            title = username.replace("-", " ").title()
        platform = "buttondown"

    # Beehiiv detection
    elif ".beehiiv.com" in hostname:
        feed_url = f"{url}/feed"
        platform = "beehiiv"

    # ConvertKit detection
    elif "convertkit.com" in hostname or ".ck.page" in hostname:
        # ConvertKit doesn't have standard RSS - try /rss
        feed_url = f"{url}/rss"
        platform = "convertkit"

    # Revue (Twitter newsletters - now defunct but some archives exist)
    elif "revue.co" in hostname or "getrevue.co" in hostname:
        username = path.lstrip("/").split("/")[0] if path else ""
        if username:
            feed_url = f"https://www.getrevue.co/profile/{username}/feed"
            title = username.replace("-", " ").title()
        platform = "revue"

    # Generic fallback - try common RSS patterns
    else:
        # Try to detect if it's already a feed URL
        if any(ext in url.lower() for ext in ["/feed", "/rss", ".xml", "/atom"]):
            feed_url = url
            platform = "direct"
        else:
            # Try adding /feed (most common pattern)
            feed_url = f"{url}/feed"
            platform = "generic"

    if not feed_url:
        return {"code": -1, "message": "Could not determine RSS feed URL.", "feed_url": None}

    logging.user(request, "~FBNewsletter convert: %s -> %s (%s)" % (url, feed_url, platform))
    return {
        "code": 1,
        "feed_url": feed_url,
        "platform": platform,
        "title": title,
        "original_url": url,
    }


@json.json_view
def podcast_search(request):
    """
    Search for podcasts using iTunes Search API.
    Returns podcasts with their RSS feed URLs.
    """
    query = request.GET.get("query", "").strip()
    limit = min(int(request.GET.get("limit", 20)), 50)

    if not query:
        return {"code": -1, "message": "Query is required", "results": []}

    # iTunes Search API (free, no auth required)
    api_url = "https://itunes.apple.com/search"
    params = {
        "term": query,
        "media": "podcast",
        "limit": limit,
        "entity": "podcast",
    }

    try:
        headers = {"User-Agent": "NewsBlur/1.0 (RSS Reader; https://newsblur.com)"}
        response = requests.get(api_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = []
        for podcast in data.get("results", []):
            # Only include podcasts that have a feed URL
            feed_url = podcast.get("feedUrl")
            if not feed_url:
                continue

            results.append({
                "name": podcast.get("collectionName", ""),
                "artist": podcast.get("artistName", ""),
                "artwork": podcast.get("artworkUrl100", ""),
                "feed_url": feed_url,
                "genre": podcast.get("primaryGenreName", ""),
                "track_count": podcast.get("trackCount", 0),
                "itunes_url": podcast.get("collectionViewUrl", ""),
            })

        logging.user(request, "~FBPodcast search for '%s': %s results" % (query, len(results)))
        return {"code": 1, "results": results, "query": query}

    except requests.exceptions.RequestException as e:
        logging.user(request, "~FRPodcast search error: %s" % str(e))
        return {"code": -1, "message": "Failed to search podcasts", "results": []}


@json.json_view
def google_news_feed(request):
    """
    Build a Google News RSS feed URL from search parameters.
    Returns the constructed RSS URL for subscribing.
    Supports both custom search queries and predefined topic feeds.
    """
    from urllib.parse import quote_plus

    query = request.GET.get("query", "").strip()
    topic = request.GET.get("topic", "").strip().upper()
    language = request.GET.get("language", "en")
    region = request.GET.get("region", "US")

    # Topic map for predefined Google News topics
    topic_map = {
        "WORLD": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx1YlY4U0FtVnVHZ0pWVXlnQVAB",
        "NATION": "CAAqIggKIhxDQkFTRHdvSkwyMHZNRFY2TVdZeUVnSmxiaWdBUAE",
        "BUSINESS": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtVnVHZ0pWVXlnQVAB",
        "TECHNOLOGY": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB",
        "ENTERTAINMENT": "CAAqJggKIiBDQkFTRWdvSUwyMHZNREpxYW5RU0FtVnVHZ0pWVXlnQVAB",
        "SPORTS": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp1ZEdvU0FtVnVHZ0pWVXlnQVAB",
        "SCIENCE": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp0Y1RjU0FtVnVHZ0pWVXlnQVAB",
        "HEALTH": "CAAqIQgKIhtDQkFTRGdvSUwyMHZNR3QwTlRFU0FtVnVLQUFQAQ",
    }

    # Build feed URL based on topic or custom query
    if topic and topic in topic_map:
        # Topic-based feed
        feed_url = f"https://news.google.com/rss/topics/{topic_map[topic]}?hl={language}&gl={region}&ceid={region}:{language}"
        title = f"Google News - {topic.title()}"
    elif query:
        # Custom search query feed
        encoded_query = quote_plus(query)
        feed_url = f"https://news.google.com/rss/search?q={encoded_query}&hl={language}&gl={region}&ceid={region}:{language}"
        title = f"Google News - {query}"
    else:
        return {"code": -1, "message": "Search query or topic is required", "feed_url": None}

    logging.user(request, "~FBGoogle News feed built: %s" % feed_url)
    return {
        "code": 1,
        "feed_url": feed_url,
        "title": title,
        "query": query,
        "topic": topic,
        "language": language,
        "region": region,
    }
